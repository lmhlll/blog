[{"categories":null,"content":"really nothing want to say! ","date":"2022-04-12","objectID":"/test_post/:0:0","tags":null,"title":"Test_post","uri":"/test_post/"},{"categories":null,"content":"Hi there, I’m a CS PhD student at Stanford. I’ve worked on Deep Learning for a few years as part of my research and among several of my related pet projects is ConvNetJS - a Javascript library for training Neural Networks. Javascript allows one to nicely visualize what’s going on and to play around with the various hyperparameter settings, but I still regularly hear from people who ask for a more thorough treatment of the topic. This article (which I plan to slowly expand out to lengths of a few book chapters) is my humble attempt. It’s on web instead of PDF because all books should be, and eventually it will hopefully include animations/demos etc. My personal experience with Neural Networks is that everything became much clearer when I started ignoring full-page, dense derivations of backpropagation equations and just started writing code. Thus, this tutorial will contain very little math (I don’t believe it is necessary and it can sometimes even obfuscate simple concepts). Since my background is in Computer Science and Physics, I will instead develop the topic from what I refer to as hackers’s perspective. My exposition will center around code and physical intuitions instead of mathematical derivations. Basically, I will strive to present the algorithms in a way that I wish I had come across when I was starting out. “…everything became much clearer when I started writing code.” You might be eager to jump right in and learn about Neural Networks, backpropagation, how they can be applied to datasets in practice, etc. But before we get there, I’d like us to first forget about all that. Let’s take a step back and understand what is really going on at the core. Lets first talk about real-valued circuits. Update note: I suspended my work on this guide a while ago and redirected a lot of my energy to teaching CS231n (Convolutional Neural Networks) class at Stanford. The notes are on cs231.github.io and the course slides can be found here. These materials are highly related to material here, but more comprehensive and sometimes more polished. ","date":"0001-01-01","objectID":"/nntutorial/:0:0","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Chapter 1: Real-valued Circuits In my opinion, the best way to think of Neural Networks is as real-valued circuits, where real values (instead of boolean values {0,1}) “flow” along edges and interact in gates. However, instead of gates such as AND, OR, NOT, etc, we have binary gates such as * (multiply), + (add), max or unary gates such as exp, etc. Unlike ordinary boolean circuits, however, we will eventually also have gradients flowing on the same edges of the circuit, but in the opposite direction. But we’re getting ahead of ourselves. Let’s focus and start out simple. ","date":"0001-01-01","objectID":"/nntutorial/:1:0","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Base Case: Single Gate in the Circuit Lets first consider a single, simple circuit with one gate. Here’s an example: x y * The circuit takes two real-valued inputs x and y and computes x * y with the * gate. Javascript version of this would very simply look something like this: var forwardMultiplyGate = function(x, y) { return x * y; }; forwardMultiplyGate(-2, 3); // returns -6. Exciting. And in math form we can think of this gate as implementing the real-valued function: $$ f(x,y) = x y $$ As with this example, all of our gates will take one or two inputs and produce a single output value. The Goal The problem we are interested in studying looks as follows: We provide a given circuit some specific input values (e.g. x = -2, y = 3) The circuit computes an output value (e.g. -6) The core question then becomes: How should one tweak the input slightly to increase the output? In this case, in what direction should we change x,y to get a number larger than -6? Note that, for example, x = -1.99 and y = 2.99 gives x * y = -5.95, which is higher than -6.0. Don’t get confused by this: -5.95 is better (higher) than -6.0. It’s an improvement of 0.05, even though the magnitude of -5.95 (the distance from zero) happens to be lower. Strategy #1: Random Local Search Okay. So wait, we have a circuit, we have some inputs and we just want to tweak them slightly to increase the output value? Why is this hard? We can easily “forward” the circuit to compute the output for any given x and y. So isn’t this trivial? Why don’t we tweak x and y randomly and keep track of the tweak that works best: // circuit with single gate for now var forwardMultiplyGate = function(x, y) { return x * y; }; var x = -2, y = 3; // some input values // try changing x,y randomly small amounts and keep track of what works best var tweak_amount = 0.01; var best_out = -Infinity; var best_x = x, best_y = y; for(var k = 0; k \u003c 100; k++) { var x_try = x + tweak_amount * (Math.random() * 2 - 1); // tweak x a bit var y_try = y + tweak_amount * (Math.random() * 2 - 1); // tweak y a bit var out = forwardMultiplyGate(x_try, y_try); if(out \u003e best_out) { // best improvement yet! Keep track of the x and y best_out = out; best_x = x_try, best_y = y_try; } } When I run this, I get best_x = -1.9928, best_y = 2.9901, and best_out = -5.9588. Again, -5.9588 is higher than -6.0. So, we’re done, right? Not quite: This is a perfectly fine strategy for tiny problems with a few gates if you can afford the compute time, but it won’t do if we want to eventually consider huge circuits with millions of inputs. It turns out that we can do much better. Stategy #2: Numerical Gradient Here’s a better way. Remember again that in our setup we are given a circuit (e.g. our circuit with a single * gate) and some particular input (e.g. x = -2, y = 3). The gate computes the output (-6) and now we’d like to tweak x and y to make the output higher. A nice intuition for what we’re about to do is as follows: Imagine taking the output value that comes out from the circuit and tugging on it in the positive direction. This positive tension will in turn translate through the gate and induce forces on the inputs x and y. Forces that tell us how x and y should change to increase the output value. What might those forces look like in our specific example? Thinking through it, we can intuit that the force on x should also be positive, because making x slightly larger improves the circuit’s output. For example, increasing x from x = -2 to x = -1 would give us output -3 - much larger than -6. On the other hand, we’d expect a negative force induced on y that pushes it to become lower (since a lower y, such as y = 2, down from the original y = 3 would make output higher: 2 x -2 = -4, again, larger than -6). That’s the intuition to keep in mind, anyway. As we go through this, it will turn out that forces I’m describing will in fact turn out to be the derivative of the output value with respect to its inputs (x and y). You may ","date":"0001-01-01","objectID":"/nntutorial/:1:1","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Recursive Case: Circuits with Multiple Gates But hold on, you say: “The analytic gradient was trivial to derive for your super-simple expression. This is useless. What do I do when the expressions are much larger? Don’t the equations get huge and complex very fast?”. Good question. Yes the expressions get much more complex. No, this doesn’t make it much harder. As we will see, every gate will be hanging out by itself, completely unaware of any details of the huge and complex circuit that it could be part of. It will only worry about its inputs and it will compute its local derivatives as seen in the previous section, except now there will be a single extra multiplication it will have to do. A single extra multiplication will turn a single (useless gate) into a cog in the complex machine that is an entire neural network. I should stop hyping it up now. I hope I’ve piqued your interest! Lets drill down into details and get two gates involved with this next example: x y z + q * f The expression we are computing now is \\( f(x,y,z) = (x + y) z \\). Lets structure the code as follows to make the gates explicit as functions: var forwardMultiplyGate = function(a, b) { return a * b; }; var forwardAddGate = function(a, b) { return a + b; }; var forwardCircuit = function(x,y,z) { var q = forwardAddGate(x, y); var f = forwardMultiplyGate(q, z); return f; }; var x = -2, y = 5, z = -4; var f = forwardCircuit(x, y, z); // output is -12 In the above, I am using a and b as the local variables in the gate functions so that we don’t get these confused with our circuit inputs x,y,z. As before, we are interested in finding the derivatives with respect to the three inputs x,y,z. But how do we compute it now that there are multiple gates involved? First, lets pretend that the + gate is not there and that we only have two variables in the circuit: q,z and a single * gate. Note that the q is is output of the + gate. If we don’t worry about x and y but only about q and z, then we are back to having only a single gate, and as far as that single * gate is concerned, we know what the (analytic) derivates are from previous section. We can write them down (except here we’re replacing x,y with q,z): $$ f(q,z) = q z \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial f(q,z)}{\\partial q} = z, \\hspace{1in} \\frac{\\partial f(q,z)}{\\partial z} = q $$ Simple enough: these are the expressions for the gradient with respect to q and z. But wait, we don’t want gradient with respect to q, but with respect to the inputs: x and y. Luckily, q is computed as a function of x and y (by addition in our example). We can write down the gradient for the addition gate as well, it’s even simpler: $$ q(x,y) = x + y \\hspace{0.5in} \\implies \\hspace{0.5in} \\frac{\\partial q(x,y)}{\\partial x} = 1, \\hspace{1in} \\frac{\\partial q(x,y)}{\\partial y} = 1 $$ That’s right, the derivatives are just 1, regardless of the actual values of x and y. If you think about it, this makes sense because to make the output of a single addition gate higher, we expect a positive tug on both x and y, regardless of their values. Backpropagation We are finally ready to invoke the Chain Rule: We know how to compute the gradient of q with respect to x and y (that’s a single gate case with + as the gate). And we know how to compute the gradient of our final output with respect to q. The chain rule tells us how to combine these to get the gradient of the final output with respect to x and y, which is what we’re ultimately interested in. Best of all, the chain rule very simply states that the right thing to do is to simply multiply the gradients together to chain them. For example, the final derivative for x will be: $$ \\frac{\\partial f(q,z)}{\\partial x} = \\frac{\\partial q(x,y)}{\\partial x} \\frac{\\partial f(q,z)}{\\partial q} $$ There are many symbols there so maybe this is confusing again, but it’s really just two numbers being multiplied together. Here is the code: // initial conditions var x = -2, y = 5, z = -4; var","date":"0001-01-01","objectID":"/nntutorial/:1:2","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Example: Single Neuron In the previous section you hopefully got the basic intuition behind backpropagation. Lets now look at an even more complicated and borderline practical example. We will consider a 2-dimensional neuron that computes the following function: $$ f(x,y,a,b,c) = \\sigma(ax + by + c) $$ In this expression, \\( \\sigma \\) is the sigmoid function. Its best thought of as a “squashing function”, because it takes the input and squashes it to be between zero and one: Very negative values are squashed towards zero and positive values get squashed towards one. For example, we have sig(-5) = 0.006, sig(0) = 0.5, sig(5) = 0.993. Sigmoid function is defined as: $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ The gradient with respect to its single input, as you can check on Wikipedia or derive yourself if you know some calculus is given by this expression: $$ \\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) (1 - \\sigma(x)) $$ For example, if the input to the sigmoid gate is x = 3, the gate will compute output f = 1.0 / (1.0 + Math.exp(-x)) = 0.95, and then the (local) gradient on its input will simply be dx = (0.95) * (1 - 0.95) = 0.0475. That’s all we need to use this gate: we know how to take an input and forward it through the sigmoid gate, and we also have the expression for the gradient with respect to its input, so we can also backprop through it. Another thing to note is that technically, the sigmoid function is made up of an entire series of gates in a line that compute more atomic functions: an exponentiation gate, an addition gate and a division gate. Treating it so would work perfectly fine but for this example I chose to collapse all of these gates into a single gate that just computes sigmoid in one shot, because the gradient expression turns out to be simple. Lets take this opportunity to carefully structure the associated code in a nice and modular way. First, I’d like you to note that every wire in our diagrams has two numbers associated with it: the value it carries during the forward pass the gradient (i.e the pull) that flows back through it in the backward pass Lets create a simple Unit structure that will store these two values on every wire. Our gates will now operate over Units: they will take them as inputs and create them as outputs. // every Unit corresponds to a wire in the diagrams var Unit = function(value, grad) { // value computed in the forward pass this.value = value; // the derivative of circuit output w.r.t this unit, computed in backward pass this.grad = grad; } In addition to Units we also need 3 gates: +, * and sig (sigmoid). Lets start out by implementing a multiply gate. I’m using Javascript here which has a funny way of simulating classes using functions. If you’re not a Javascript - familiar person, all that’s going on here is that I’m defining a class that has certain properties (accessed with use of this keyword), and some methods (which in Javascript are placed into the function’s prototype). Just think about these as class methods. Also keep in mind that the way we will use these eventually is that we will first forward all the gates one by one, and then backward all the gates in reverse order. Here is the implementation: var multiplyGate = function(){ }; multiplyGate.prototype = { forward: function(u0, u1) { // store pointers to input Units u0 and u1 and output unit utop this.u0 = u0; this.u1 = u1; this.utop = new Unit(u0.value * u1.value, 0.0); return this.utop; }, backward: function() { // take the gradient in output unit and chain it with the // local gradients, which we derived for multiply gate before // then write those gradients to those Units. this.u0.grad += this.u1.value * this.utop.grad; this.u1.grad += this.u0.value * this.utop.grad; } } The multiply gate takes two units that each hold a value and creates a unit that stores its output. The gradient is initialized to zero. Then notice that in the backward function call we get the gradient from the output unit we produced dur","date":"0001-01-01","objectID":"/nntutorial/:1:3","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Becoming a Backprop Ninja Over time you will become much more efficient in writing the backward pass, even for complicated circuits and all at once. Lets practice backprop a bit with a few examples. In what follows, lets not worry about Unit, Circuit classes because they obfuscate things a bit, and lets just use variables such as a,b,c,x, and refer to their gradients as da,db,dc,dx respectively. Again, we think of the variables as the “forward flow” and their gradients as “backward flow” along every wire. Our first example was the * gate: var x = a * b; // and given gradient on x (dx), we saw that in backprop we would compute: var da = b * dx; var db = a * dx; In the code above, I’m assuming that the variable dx is given, coming from somewhere above us in the circuit while we’re doing backprop (or it is +1 by default otherwise). I’m writing it out because I want to explicitly show how the gradients get chained together. Note from the equations that the * gate acts as a switcher during backward pass, for lack of better word. It remembers what its inputs were, and the gradients on each one will be the value of the other during the forward pass. And then of course we have to multiply with the gradient from above, which is the chain rule. Here’s the + gate in this condensed form: var x = a + b; // -\u003e var da = 1.0 * dx; var db = 1.0 * dx; Where 1.0 is the local gradient, and the multiplication is our chain rule. What about adding three numbers?: // lets compute x = a + b + c in two steps: var q = a + b; // gate 1 var x = q + c; // gate 2 // backward pass: dc = 1.0 * dx; // backprop gate 2 dq = 1.0 * dx; da = 1.0 * dq; // backprop gate 1 db = 1.0 * dq; You can see what’s happening, right? If you remember the backward flow diagram, the + gate simply takes the gradient on top and routes it equally to all of its inputs (because its local gradient is always simply 1.0 for all its inputs, regardless of their actual values). So we can do it much faster: var x = a + b + c; var da = 1.0 * dx; var db = 1.0 * dx; var dc = 1.0 * dx; Okay, how about combining gates?: var x = a * b + c; // given dx, backprop in-one-sweep would be =\u003e da = b * dx; db = a * dx; dc = 1.0 * dx; If you don’t see how the above happened, introduce a temporary variable q = a * b and then compute x = q + c to convince yourself. And here is our neuron, lets do it in two steps: // lets do our neuron in two steps: var q = a*x + b*y + c; var f = sig(q); // sig is the sigmoid function // and now backward pass, we are given df, and: var df = 1; var dq = (f * (1 - f)) * df; // and now we chain it to the inputs var da = x * dq; var dx = a * dq; var dy = b * dq; var db = y * dq; var dc = 1.0 * dq; I hope this is starting to make a little more sense. Now how about this: var x = a * a; var da = //??? You can think of this as value a flowing to the * gate, but the wire gets split and becomes both inputs. This is actually simple because the backward flow of gradients always adds up. In other words nothing changes: var da = a * dx; // gradient into a from first branch da += a * dx; // and add on the gradient from the second branch // short form instead is: var da = 2 * a * dx; In fact, if you know your power rule from calculus you would also know that if you have \\( f(a) = a^2 \\) then \\( \\frac{\\partial f(a)}{\\partial a} = 2a \\), which is exactly what we get if we think of it as wire splitting up and being two inputs to a gate. Lets do another one: var x = a*a + b*b + c*c; // we get: var da = 2*a*dx; var db = 2*b*dx; var dc = 2*c*dx; Okay now lets start to get more complex: var x = Math.pow(((a * b + c) * d), 2); // pow(x,2) squares the input JS When more complex cases like this come up in practice, I like to split the expression into manageable chunks which are almost always composed of simpler expressions and then I chain them together with chain rule: var x1 = a * b + c; var x2 = x1 * d; var x = x2 * x2; // this is identical to the above expression for x // and now in backprop we go ","date":"0001-01-01","objectID":"/nntutorial/:1:4","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Chapter 2: Machine Learning In the last chapter we were concerned with real-valued circuits that computed possibly complex expressions of their inputs (the forward pass), and also we could compute the gradients of these expressions on the original inputs (backward pass). In this chapter we will see how useful this extremely simple mechanism is in Machine Learning. ","date":"0001-01-01","objectID":"/nntutorial/:2:0","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Binary Classification As we did before, lets start out simple. The simplest, common and yet very practical problem in Machine Learning is binary classification. A lot of very interesting and important problems can be reduced to it. The setup is as follows: We are given a dataset of N vectors and every one of them is labeled with a +1 or a -1. For example, in two dimensions our dataset could look as simple as: vector -\u003e label --------------- [1.2, 0.7] -\u003e +1 [-0.3, 0.5] -\u003e -1 [-3, -1] -\u003e +1 [0.1, 1.0] -\u003e -1 [3.0, 1.1] -\u003e -1 [2.1, -3] -\u003e +1 Here, we have N = 6 datapoints, where every datapoint has two features (D = 2). Three of the datapoints have label +1 and the other three label -1. This is a silly toy example, but in practice a +1/-1 dataset could be very useful things indeed: For example spam/no spam emails, where the vectors somehow measure various features of the content of the email, such as the number of times certain enhancement drugs are mentioned. Goal. Our goal in binary classification is to learn a function that takes a 2-dimensional vector and predicts the label. This function is usually parameterized by a certain set of parameters, and we will want to tune the parameters of the function so that its outputs are consistent with the labeling in the provided dataset. In the end we can discard the dataset and use the learned parameters to predict labels for previously unseen vectors. Training protocol We will eventually build up to entire neural networks and complex expressions, but lets start out simple and train a linear classifier very similar to the single neuron we saw at the end of Chapter 1. The only difference is that we’ll get rid of the sigmoid because it makes things unnecessarily complicated (I only used it as an example in Chapter 1 because sigmoid neurons are historically popular but modern Neural Networks rarely, if ever, use sigmoid non-linearities). Anyway, lets use a simple linear function: $$ f(x, y) = ax + by + c $$ In this expression we think of x and y as the inputs (the 2D vectors) and a,b,c as the parameters of the function that we will want to learn. For example, if a = 1, b = -2, c = -1, then the function will take the first datapoint ([1.2, 0.7]) and output 1 * 1.2 + (-2) * 0.7 + (-1) = -1.2. Here is how the training will work: We select a random datapoint and feed it through the circuit We will interpret the output of the circuit as a confidence that the datapoint has class +1. (i.e. very high values = circuit is very certain datapoint has class +1 and very low values = circuit is certain this datapoint has class -1.) We will measure how well the prediction aligns with the provided labels. Intuitively, for example, if a positive example scores very low, we will want to tug in the positive direction on the circuit, demanding that it should output higher value for this datapoint. Note that this is the case for the the first datapoint: it is labeled as +1 but our predictor unction only assigns it value -1.2. We will therefore tug on the circuit in positive direction; We want the value to be higher. The circuit will take the tug and backpropagate it to compute tugs on the inputs a,b,c,x,y Since we think of x,y as (fixed) datapoints, we will ignore the pull on x,y. If you’re a fan of my physical analogies, think of these inputs as pegs, fixed in the ground. On the other hand, we will take the parameters a,b,c and make them respond to their tug (i.e. we’ll perform what we call a parameter update). This, of course, will make it so that the circuit will output a slightly higher score on this particular datapoint in the future. Iterate! Go back to step 1. The training scheme I described above, is commonly referred as Stochastic Gradient Descent. The interesting part I’d like to reiterate is that a,b,c,x,y are all made up of the same stuff as far as the circuit is concerned: They are inputs to the circuit and the circuit will tug on all of them in some direction. It doesn’t know the difference betwee","date":"0001-01-01","objectID":"/nntutorial/:2:1","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"A more Conventional Approach: Loss Functions Now that we understand the basics of how these circuits function with data, lets adopt a more conventional approach that you might see elsewhere on the internet and in other tutorials and books. You won’t see people talking too much about force specifications. Instead, Machine Learning algorithms are specified in terms of loss functions (or cost functions, or objectives). As I develop this formalism I would also like to start to be a little more careful with how we name our variables and parameters. I’d like these equations to look similar to what you might see in a book or some other tutorial, so let me use more standard naming conventions. Example: 2-D Support Vector Machine Lets start with an example of a 2-dimensional SVM. We are given a dataset of \\( N \\) examples \\( (x_{i0}, x_{i1}) \\) and their corresponding labels \\( y_{i} \\) which are allowed to be either \\( +1/-1 \\) for positive or negative example respectively. Most importantly, as you recall we have three parameters \\( (w_0, w_1, w_2) \\). The SVM loss function is then defined as follows: $$ L = [\\sum_{i=1}^N max(0, -y_{i}( w_0x_{i0} + w_1x_{i1} + w_2 ) + 1 )] + \\alpha [w_0^2 + w_1^2] $$ Notice that this expression is always positive, due to the thresholding at zero in the first expression and the squaring in the regularization. The idea is that we will want this expression to be as small as possible. Before we dive into some of its subtleties let me first translate it to code: var X = [ [1.2, 0.7], [-0.3, 0.5], [3, 2.5] ] // array of 2-dimensional data var y = [1, -1, 1] // array of labels var w = [0.1, 0.2, 0.3] // example: random numbers var alpha = 0.1; // regularization strength function cost(X, y, w) { var total_cost = 0.0; // L, in SVM loss function above N = X.length; for(var i=0;i\u003cN;i++) { // loop over all data points and compute their score var xi = X[i]; var score = w[0] * xi[0] + w[1] * xi[1] + w[2]; // accumulate cost based on how compatible the score is with the label var yi = y[i]; // label var costi = Math.max(0, - yi * score + 1); console.log('example ' + i + ': xi = (' + xi + ') and label = ' + yi); console.log(' score computed to be ' + score.toFixed(3)); console.log(' =\u003e cost computed to be ' + costi.toFixed(3)); total_cost += costi; } // regularization cost: we want small weights reg_cost = alpha * (w[0]*w[0] + w[1]*w[1]) console.log('regularization cost for current model is ' + reg_cost.toFixed(3)); total_cost += reg_cost; console.log('total cost is ' + total_cost.toFixed(3)); return total_cost; } And here is the output: cost for example 0 is 0.440 cost for example 1 is 1.370 cost for example 2 is 0.000 regularization cost for current model is 0.005 total cost is 1.815 Notice how this expression works: It measures how bad our SVM classifier is. Lets step through this explicitly: The first datapoint xi = [1.2, 0.7] with label yi = 1 will give score 0.1*1.2 + 0.2*0.7 + 0.3, which is 0.56. Notice, this is a positive example so we want to the score to be greater than +1. 0.56 is not enough. And indeed, the expression for cost for this datapoint will compute: costi = Math.max(0, -1*0.56 + 1), which is 0.44. You can think of the cost as quantifying the SVM’s unhappiness. The second datapoint xi = [-0.3, 0.5] with label yi = -1 will give score 0.1*(-0.3) + 0.2*0.5 + 0.3, which is 0.37. This isn’t looking very good: This score is very high for a negative example. It should be less than -1. Indeed, when we compute the cost: costi = Math.max(0, 1*0.37 + 1), we get 1.37. That’s a very high cost from this example, as it is being misclassified. The last example xi = [3, 2.5] with label yi = 1 gives score 0.1*3 + 0.2*2.5 + 0.3, and that is 1.1. In this case, the SVM will compute costi = Math.max(0, -1*1.1 + 1), which is in fact zero. This datapoint is being classified correctly and there is no cost associated with it. A cost function is an expression that measuress how bad your classifier is. When the training set","date":"0001-01-01","objectID":"/nntutorial/:2:2","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Chapter 3: Backprop in Practice ","date":"0001-01-01","objectID":"/nntutorial/:3:0","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Building up a library ","date":"0001-01-01","objectID":"/nntutorial/:3:1","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Example: Practical Neural Network Classifier Multiclass: Structured SVM Multiclass: Logistic Regression, Softmax ","date":"0001-01-01","objectID":"/nntutorial/:3:2","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Example: Regression Tiny changes needed to cost function. L2 regularization. ","date":"0001-01-01","objectID":"/nntutorial/:3:3","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Example: Structured Prediction Basic idea is to train an (unnormalized) energy model ","date":"0001-01-01","objectID":"/nntutorial/:3:4","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Vectorized Implementations Writing a Neural Net classfier in Python with numpy…. ","date":"0001-01-01","objectID":"/nntutorial/:3:5","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Backprop in practice: Tips/Tricks Monitoring of Cost function Monitoring training/validation performance Tweaking initial learning rates, learning rate schedules Optimization: Using Momentum Optimization: LBFGS, Nesterov accelerated gradient Importance of Initialization: weights and biases Regularization: L2, L1, Group sparsity, Dropout Hyperparameter search, cross-validations Common pitfalls: (e.g. dying ReLUs) Handling unbalanced datasets Approaches to debugging nets when something doesnt work ","date":"0001-01-01","objectID":"/nntutorial/:3:6","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Chapter 4: Networks in the Wild Case studies of models that work well in practice and have been deployed in the wild. ","date":"0001-01-01","objectID":"/nntutorial/:4:0","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Case Study: Convolutional Neural Networks for images Convolutional layers, pooling, AlexNet, etc. ","date":"0001-01-01","objectID":"/nntutorial/:4:1","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Case Study: Recurrent Neural Networks for Speech and Text Vanilla Recurrent nets, bi-directional recurrent nets. Maybe overview of LSTM ","date":"0001-01-01","objectID":"/nntutorial/:4:2","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Case Study: Word2Vec Training word vector representations in NLP ","date":"0001-01-01","objectID":"/nntutorial/:4:3","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Case Study: t-SNE Training embeddings for visualizing data ","date":"0001-01-01","objectID":"/nntutorial/:4:4","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Acknowledgements Thanks a lot to the following people who made this guide better: wodenokoto (HN), zackmorris (HN). ","date":"0001-01-01","objectID":"/nntutorial/:5:0","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"},{"categories":null,"content":"Comments This guide is a work in progress and I appreciate feedback, especially regarding parts that were unclear or only made half sense. Thank you! Some of the Javascript code in this tutorial has been translated to Python by Ajit, find it over on Github. ","date":"0001-01-01","objectID":"/nntutorial/:6:0","tags":null,"title":"Hacker's guide to Neural Networks","uri":"/nntutorial/"}]